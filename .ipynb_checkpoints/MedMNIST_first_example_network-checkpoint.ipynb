{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6946ea",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d43353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e676e",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "`dataset`: gives description of dataset\\\n",
    "`dataset[n][0]` the image of patient n\\\n",
    "`dataset[n][1]` binary classification class: `[0]` = normal, `[1]` = pneumonia\n",
    "\n",
    "In this section, we import the 'test' section of the dataset for visualisation. **Note:** None of these variables used after this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0074d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from medmnist import PneumoniaMNIST\n",
    "dataset = PneumoniaMNIST(split='test',download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal=0\n",
    "pneumonia=0\n",
    "for image in dataset: \n",
    "    if image[1] == [0]: \n",
    "        normal+=1\n",
    "    elif image[1] == [1]: \n",
    "        pneumonia+=1\n",
    "\n",
    "plt.bar(('normal','pneumonia'),(normal,pneumonia))\n",
    "plt.title('Number of pneumonia-positive patients in dataset')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(dataset[np.random.randint(len(dataset))][0],cmap='gray') #show random patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc9276",
   "metadata": {},
   "source": [
    "## Dataset Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3897cac",
   "metadata": {},
   "source": [
    "`INFO` method of `medmnist` provides all information about the dataset, type=dictionary. We extract some of the information from this for later use in the training process. \n",
    "\n",
    "Hyperparameters -- crucial to control for good results:\n",
    "1. `NUM_EPOCHS`: number of times the neural network is trained on the entire dataset.\n",
    "2. `BATCH_SIZE`: number of images before parameters of the NN are updates.\n",
    "3. `lr`: learning rate, controls how much the network's parameters are adjusted based on the errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b1097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'pneumoniamnist'\n",
    "download = False\n",
    "\n",
    "NUM_EPOCHS = 3 #no. of times the NN is trained on the entire dataset\n",
    "BATCH_SIZE = 32 #no. of images before parameters are updated\n",
    "lr = 0.001 \n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task'] #binary classification or multi-classification?\n",
    "n_channels = info['n_channels'] #colour channels\n",
    "n_classes = len(info['label']) #number of classes\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68460bb",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Define data transformer to transform PIL images into a tensor format, and apply a normalisation for better performance. \n",
    "2. Use the `DataClass` method defined above to create training and test dataset.\n",
    "3. `DataLoader`: Split data into batches and shuffle to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b754f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4b4524371d85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#1. \n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# 2. \n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# 3. \n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f36448",
   "metadata": {},
   "source": [
    "## CNN Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0370839",
   "metadata": {},
   "source": [
    "First, define layers of CNN. Sequential Structure. \n",
    "\n",
    "1. `nn.Conv2d:` Applies 2D convolutions to extract features from images.\n",
    "2. `nn.BatchNorm2d:` Normalizes input to stabilize training.\n",
    "3. `nn.ReLU:` Type of activation function: introduces non-linearity for better learning.\n",
    "4. `nn.MaxPool2d:` Downsamples feature maps to reduce parameters and computational cost.\n",
    "\n",
    "Class `Net` inherits all functions from PyTorch `nn.module`. \n",
    "\n",
    "Next, define the optimiser and loss function.\n",
    "1. **Loss Function**: A loss function measures the difference between the model's predictions and the actual labels. Common choice is Cross-Entropy Loss -- penalizes the model for making incorrect predictions, considering the probability distribution of the predicted classes. \n",
    "2. **Optimiser:**  updates the model's internal parameters (weights and biases) based on the calculated loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48509c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes) #build model\n",
    "    \n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ee17a",
   "metadata": {},
   "source": [
    "## Training\n",
    "*How does the training process work?*\n",
    "1. Model takes a batch of images and their corresponding labels.\n",
    "2. It predicts class probabilities for each image.\n",
    "3. The loss function calculates the difference between these predictions and the true labels.\n",
    "4. The gradients of the loss function with respect to each model parameter are computed (backpropagation).\n",
    "5. The chosen optimizer uses these gradients to update the model's parameters in a direction that reduces the loss.\n",
    "6. This process repeats for each batch of data over multiple epochs (complete passes through the entire dataset).\n",
    "*Code*: \n",
    "For each epoch: \n",
    "1. Set model to traning mode\n",
    "2. Refer back to `train_loader` variable: this is the training dataset split into a number of each batches. Code loops through each batch. `inputs` = input images, `targets` = image label\n",
    "3. For each batch, set optimiser to zero. Feed inputs through the network, and evaluate the loss function. Perform back propagation then update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36872974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:02<00:00, 52.76it/s]\n",
      "100%|██████████| 148/148 [00:02<00:00, 53.76it/s]\n",
      "100%|██████████| 148/148 [00:02<00:00, 56.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() #set model to 'training mode'\n",
    "    for inputs, targets in tqdm(train_loader): #tqdm is progress bar. \n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        outputs = model(inputs) # send inputs through the network, calculate output label\n",
    "        targets = targets.squeeze().long()\n",
    "        loss = criterion(outputs, targets) #compare predicted outputs to targets using loss function\n",
    "        loss.backward() #back propagation \n",
    "        optimizer.step() #update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b463af",
   "metadata": {},
   "source": [
    "## Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c07da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  auc: 0.966  acc:0.816\n"
     ]
    }
   ],
   "source": [
    "def test(split):\n",
    "    model.eval() #set model to 'evaluation' mode\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    \n",
    "    data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "\n",
    "    with torch.no_grad(): #deactivate gradient for efficiency.\n",
    "        for inputs, targets in data_loader: #loop through batch\n",
    "            outputs = model(inputs) #prediction\n",
    "            \n",
    "            #process target and outputs\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "            \n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.numpy()\n",
    "        y_score = y_score.detach().numpy()\n",
    "        \n",
    "        evaluator = Evaluator(data_flag, split)\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "\n",
    "        \n",
    "test('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef9fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
